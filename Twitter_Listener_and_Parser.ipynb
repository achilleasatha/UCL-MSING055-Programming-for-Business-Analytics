{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import locale\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.sQtreaming import StreamListener\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Request from Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXx'\n",
    "consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_token_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "class listener(StreamListener):\n",
    "    def on_data(self,data):\n",
    "        try:\n",
    "            with open('twitDB.json','a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException:\n",
    "            print ('failed ondata,')\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def on_error(self,status):\n",
    "        print(status)\n",
    "\n",
    "auth=OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "twitterStream= Stream(auth,listener())\n",
    "#Filters based on which Tweets are selected\n",
    "twitterStream.filter(track=[\"#BlackFriday\"], languages=[\"en\"], locations=[-11.500344, 49.907747, 2.668433, 58.850078])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create lists with  SELECTED tweets info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lists(file):\n",
    "    i=0\n",
    "    j=0\n",
    "    k=0\n",
    "    l=0\n",
    "    date=[]\n",
    "    tweet_id=[]\n",
    "    text=[]\n",
    "    retweet=[]\n",
    "    quote=[]\n",
    "    reply=[]\n",
    "    favorite_count=[]\n",
    "    user_id=[]\n",
    "    user_name=[]\n",
    "    followers=[]\n",
    "    friends=[]\n",
    "    original_tw_id=[]\n",
    "    original_tw=[]\n",
    "    original_id=[]\n",
    "    original_foll=[]\n",
    "    time=[]\n",
    "    isretweet=[]\n",
    "    while i < len(file):\n",
    "        try:\n",
    "            #tweet information\n",
    "            date.append(datetime.strptime((file[i][\"created_at\"]), '%a %b %d %H:%M:%S +0000 %Y').date())\n",
    "            time.append(datetime.strptime((file[i][\"created_at\"]), '%a %b %d %H:%M:%S +0000 %Y').time())\n",
    "            tweet_id.append(file[i][\"id_str\"])\n",
    "            text.append(file[i][\"text\"])\n",
    "            #user information\n",
    "            user_id.append(file[i][\"user\"][\"id_str\"])\n",
    "            user_name.append(file[i][\"user\"][\"screen_name\"])\n",
    "            followers.append(file[i][\"user\"][\"followers_count\"])\n",
    "            friends.append(file[i][\"user\"][\"friends_count\"])\n",
    "            #retweet info\n",
    "            if \"retweeted_status\" in file[i]:\n",
    "                original_tw_id.append(file[i][\"retweeted_status\"][\"id_str\"])\n",
    "                original_tw.append(file[i][\"retweeted_status\"][\"text\"])\n",
    "                original_id.append(file[i][\"retweeted_status\"][\"user\"][\"id_str\"])\n",
    "                original_foll.append(file[i][\"retweeted_status\"][\"user\"][\"followers_count\"])\n",
    "                retweet.append(file[i][\"retweeted_status\"][\"retweet_count\"])\n",
    "                quote.append(file[i][\"retweeted_status\"][\"quote_count\"])\n",
    "                reply.append(file[i][\"retweeted_status\"][\"reply_count\"])\n",
    "                favorite_count.append(file[i][\"retweeted_status\"][\"favorite_count\"])\n",
    "                isretweet.append(int(1))\n",
    "                l+=1\n",
    "            else:\n",
    "                original_tw_id.append(\"NaN\")\n",
    "                original_tw.append(\"NaN\")\n",
    "                original_id.append(\"NaN\")\n",
    "                original_foll.append(\"NaN\")\n",
    "                retweet.append(\"NaN\")\n",
    "                quote.append(\"NaN\")\n",
    "                reply.append(\"NaN\")\n",
    "                favorite_count.append(\"NaN\")\n",
    "                isretweet.append(int(0))\n",
    "                k+=1\n",
    "        except KeyError:\n",
    "            if \"retweeted_status\" in file[i]:\n",
    "                isretweet.append(int(1))\n",
    "            else:\n",
    "                isretweet.append(int(0))\n",
    "            if \"created_at\" not in file[i]:\n",
    "                date.append(\"NaN\")\n",
    "                time.append(\"NaN\")\n",
    "                #print(\"Null created at values\")\n",
    "            else:\n",
    "                date.append(datetime.strptime((file[i][\"created_at\"]), '%a %b %d %H:%M:%S +0000 %Y').date())\n",
    "                time.append(datetime.strptime((file[i][\"created_at\"]), '%a %b %d %H:%M:%S +0000 %Y').time())      \n",
    "            if \"id_str\" not in file[i]:   \n",
    "                    tweet_id.append(\"NaN\")\n",
    "                    #print(\"ID String Error\")\n",
    "            else:\n",
    "                tweet_id.append(file[i][\"id_str\"])\n",
    "            if \"text\" not in file[i]:      \n",
    "                    text.append(\"NaN\")\n",
    "                    #print(\"Tweet ID Error\")\n",
    "            else:\n",
    "                text.append(file[i][\"text\"])\n",
    "            #user information\n",
    "            if \"user\"\"id_str\" not in file[i]:  \n",
    "                    user_id.append(\"NaN\")\n",
    "                    #print(\"User ID String Error\")\n",
    "            else:\n",
    "                user_id.append(file[i][\"user\"][\"id_str\"])\n",
    "            if \"user\"\"screen_name\" not in file[i]:       \n",
    "                    user_name.append(\"NaN\")\n",
    "                    #print(\"User Name Error\")\n",
    "            else:\n",
    "                user_name.append(file[i][\"user\"][\"screen_name\"])\n",
    "            if \"user\"\"followers_count\" not in file[i]:       \n",
    "                    followers.append(\"NaN\")\n",
    "                    #print(\"Follower count Error\")\n",
    "            else:\n",
    "                followers.append(file[i][\"user\"][\"followers_count\"])\n",
    "            if \"user\"\"friends_count\" not in file[i]:     \n",
    "                    friends.append(\"NaN\")\n",
    "                    #print(\"Friends Error\")\n",
    "            else:\n",
    "                friends.append(file[i][\"user\"][\"friends_count\"])\n",
    "            if \"retweeted_status\"\"id_str\" not in file[i]:\n",
    "                original_tw_id.append(\"NaN\")\n",
    "                #print(\"Original tweet ID Error\")\n",
    "            else:\n",
    "                original_tw_id.append(file[i][\"retweeted_status\"][\"id_str\"])           \n",
    "            if \"retweeted_status\"\"text\" not in file[i]:\n",
    "                original_tw.append(\"NaN\")\n",
    "                #print(\"Original Tweet Text Error\")\n",
    "            else:\n",
    "                original_tw.append(file[i][\"retweeted_status\"][\"text\"])\n",
    "            if \"retweeted_status\"\"user\"\"id_str\" not in file[i]:\n",
    "                original_id.append(\"NaN\")\n",
    "                #print(\"Original ID Error\")\n",
    "            else:\n",
    "                original_id.append(file[i][\"retweeted_status\"][\"user\"][\"id_str\"])\n",
    "            if \"retweeted_status\"\"user\"\"followers_count\" not in file[i]:\n",
    "                original_foll.append(\"NaN\")\n",
    "                #print(\"Original Follower Error\")\n",
    "            else:\n",
    "                original_foll.append(file[i][\"retweeted_status\"][\"user\"][\"followers_count\"])\n",
    "            if \"retweeted_status\"\"retweet_count\" not in file[i]:\n",
    "                retweet.append(\"NaN\")\n",
    "                #print(\"Original Retweet Count Error\")\n",
    "            else:\n",
    "                retweet.append(file[i][\"retweeted_status\"][\"retweet_count\"])\n",
    "            if \"retweeted_status\"\"quote_count\" not in file[i]:\n",
    "                quote.append(\"NaN\")\n",
    "                #print(\"Original Quote Count Error\")\n",
    "            else:\n",
    "                quote.append(file[i][\"retweeted_status\"][\"quote_count\"])\n",
    "            if \"retweeted_status\"\"reply_count\" not in file[i]:\n",
    "                reply.append(\"NaN\")\n",
    "                #print(\"Original Reply Count Error\")\n",
    "            else:\n",
    "                reply.append(file[i][\"retweeted_status\"][\"reply_count\"])\n",
    "            if \"retweeted_status\"\"favorite_count\" not in file[i]:\n",
    "                favorite_count.append(\"NaN\")\n",
    "                #print(\"Original Count Error\")\n",
    "            else:\n",
    "                favorite_count.append(file[i][\"retweeted_status\"][\"favorite_count\"])\n",
    "            j+=1 #counts how many instances of this error have occured\n",
    "        i+=1\n",
    "    #pass attributes to list    \n",
    "    lists_file=[date,time, tweet_id,text,retweet, quote, reply, favorite_count, user_id, user_name,followers,friends,original_tw_id, original_tw, original_id, original_foll, isretweet]\n",
    "    print(\"Processed Tweets: \", len(file))\n",
    "    print(\"Total Errors Occured: \", j)\n",
    "    print(\"Total Tweets: \", k) \n",
    "    print(\"Total Retweets: \", l)\n",
    "    return lists_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Json file with tweets info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100000 line.\n",
      "Processing 200000 line.\n",
      "Processing 300000 line.\n",
      "Processing 400000 line.\n",
      "Processing 500000 line.\n",
      "Processing 600000 line.\n",
      "Processing 700000 line.\n",
      "--- Successful run ---\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "m=0\n",
    "with open('twitDB1230.json') as f:\n",
    "    for line in f:\n",
    "        m+=1\n",
    "        if (m % 100000) == 0:\n",
    "                print(\"Processing %d line.\" %(m))\n",
    "                #break #break after 100,000 tweets processed REMOVE THIS FOR FULL RUN\n",
    "        data.append(json.loads(line))\n",
    "    print(\"--- Successful run ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lists of selected information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tweets:  785491\n",
      "Total Errors Occured:  23863\n",
      "Total Tweets:  339875\n",
      "Total Retweets:  421753\n"
     ]
    }
   ],
   "source": [
    "lists_file= create_lists(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Frame from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761628\n",
      "421753\n",
      "339875\n"
     ]
    }
   ],
   "source": [
    "df= pd.DataFrame(\n",
    "    {\"date\":lists_file[0],\n",
    "     \"time\":lists_file[1],\n",
    "     \"tweet_id\": lists_file[2],\n",
    "     \"tweet_text\":lists_file[3],\n",
    "     \"retweets\":lists_file[4],\n",
    "     \"quote\":lists_file[5],\n",
    "     \"replies\":lists_file[6],\n",
    "     \"favorite\":lists_file[7],\n",
    "     \"user_id\":lists_file[8],\n",
    "     \"user_name\":lists_file[9],\n",
    "     \"followers\":lists_file[10],\n",
    "     \"friends\":lists_file[11],\n",
    "     \"orig_tweet_id\":lists_file[12],\n",
    "     \"orig_tweet\":lists_file[13],\n",
    "     \"orig_userid\":lists_file[14],\n",
    "     \"orig_follow\":lists_file[15],\n",
    "     \"isretweet\":lists_file[16]\n",
    "     })\n",
    "#Drop all NaN date rows (and consequently any rows that contain corrupt data)\n",
    "df=df[df.date != 'NaN']\n",
    "df2=df\n",
    "#Create duplicate dataframe to parse out duplicate records of retweets\n",
    "df2=df2[df2.orig_tweet_id != \"NaN\"]\n",
    "df3=df\n",
    "df3=df3[df3.orig_tweet_id == \"NaN\"]\n",
    "#print(df.date.count())\n",
    "#print(df2.date.count())\n",
    "#print(df3.date.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67193\n"
     ]
    }
   ],
   "source": [
    "#Keep only unique original id str with the highest number of retweets\n",
    "temp_df=df2.sort_values(['retweets'], ascending=False).drop_duplicates(subset=[\"orig_tweet_id\"], keep=\"first\")\n",
    "#print(temp_df.date.count())\n",
    "#temp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407068\n"
     ]
    }
   ],
   "source": [
    "#recombine all dataframes\n",
    "frames = [temp_df, df3]\n",
    "result = pd.concat(frames)\n",
    "#print(result.date.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-543d2d3551c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "#Test results\n",
    "result.head(5)\n",
    "df.iloc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump results to CSV files (one for tweets with unique IDs and one for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pass results to CSV\n",
    "result.iloc[:].to_csv('UniqueIDs.csv')\n",
    "df.iloc[:].to_csv('AllTweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counting across all tweet \"text\" objects for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read JSON file\n",
    "with open('../twitDB1230.json') as f:\n",
    "    print('File opened')\n",
    "    for line in f:\n",
    "        with open('tweet_texts4.txt','a') as tweetTextContent:\n",
    "            line = json.loads(line)\n",
    "            if (not hasattr(line,'retweeted_status')):\n",
    "                try:\n",
    "                    tweetTextContent.write(line['text'])\n",
    "                except KeyError:\n",
    "                    print('This tweet does not have any text')\n",
    "            else:\n",
    "                print('Retweet, ignore this')\n",
    "#Fetch Commonly used words\n",
    "with open('tweet_texts4.txt') as allTweetTexts:\n",
    "    tweets = allTweetTexts.read()\n",
    "    \n",
    "#Make a list of unique words\n",
    "words = re.findall(r'\\w+', tweets)\n",
    "\n",
    "#Uppercase the words so they are comparable\n",
    "cap_words = [word.upper() for word in words]\n",
    "\n",
    "word_counts = Counter(cap_words)\n",
    "print(word_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
